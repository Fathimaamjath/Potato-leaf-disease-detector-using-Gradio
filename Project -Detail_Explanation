POTATO LEAF DISEASE DETECTOR 

PROBLEM STATEMENT: To develop a model for detecting diseases affecting potato plant and implementing a simple user interface to examine the detector.

PROJECT EXPLANATION: Using this model we are detecting healthy plant leaves and two diseases affected leaves of potato plants.

1.Early blight  disease - Spots begin as small, dark, dry, papery flecks, which grow to become brown-black, circular-to-oval areas. 

2. Late Blight disease- Leaf spots begin as small, pale to dark green,   irregularly shaped spots.

PROJECT IMPLEMENTATION: 
 
  Step 1: Import necessary libraries/Modules
  
  Step 2: Loading the dataset 
  
  Step 3: Dataset visualization
  
  Step 4: Data preprocessing
  
  Step 5: Model Training
  
  Step 6: Implementing a simple user interface

MODULES EXPLANATION:

  [1] TensorFlow - libraries for building and training machine learning and deep learning models. 

  [2] Models and layers  module from TensorFlow keras API – These modules allow to create, customize and train various neural network architecture for ML and DL task. 

  [3] Matplotlib is a popular and versatile Python library for creating interactive visualizations in many fields. It provides a wide range of tools for creating high-quality plots, charts, and figures.

  [4] Gradio is an open-source Python library used for creating interactive and customizable machine learning interfaces. It is used to build web-based interfaces for machine learning models and  making it easier for users to interact and understand the models.

IMPLEMENTATION [Step by step explanation]

Step 1: Import libraries

Step 2: Import dataset

Dataset used is ‘PlantVillage’ dataset. It is a popular and publicly available dataset in the field of plant pathology. It consists of a large collection of images of plant leaves, each of which is associated with specific plant diseases.

tf.keras.preprocessing.image_dataset_from_directory - reads the images from  specified directory, infers labels from subdirectories, resizes images to a specific size, and creates a dataset suitable for training or validation.

Step 3: To view the images in each dataset we use matplotlib library.

Step 4: Data - preprocessing

Create a function to divide the dataset into training, validation, and test sets for machine learning purposes.
It takes the input dataset and specified proportion for each.
The function shuffles the dataset using specified shuffle size.
Finally the function returns the three partitioned dataset: train_ds, val_ds, test_ds.

Inorder to improve data loading efficiency and preprocessing we use two methods: Cache and prefetch.

cache(): Caching a dataset means storing its elements (or a portion of them) in memory, which can help in speeding up data loading and preprocessing. It avoids reading the same data from storage repeatedly when the dataset is too large.

Prefetch: It allows the dataset to load and prepare batches of data in the background while your model is training on the current batch. 

The `buffer_size` specifies how many batches should be prefetched. 

`tf.data.AUTOTUNE` is a special value that lets TensorFlow automatically choose an appropriate buffer size based on the available system resources, making data loading as efficient as possible.

Performance Optimization:For attaining this we create a sequential model with two preprocessing layers:

1. Resizing: It resizes input images to the specified dimensions.
2. Rescaling: It scales the pixel values of the images to the range [0, 1].

These preprocessing steps are often used as the initial part of a neural network model for image classification tasks to prepare the data before it is fed into the neural network layers for training or inference.

Data Augmentation:It is often used in combination with other techniques like dropout and regularization to reduce overfitting and improve model performance. It helps the model become more invariant to variations in the input data, leading to better generalization.
Flipping : flipping the image in both horizontal and vertical directions.
Rotating: rotating the image by a certain angle and rotated up to 0.2 radian(11.5 degrees) in either direction. 

Step 4: Model Building

In this project we are using Convolutional neural networks(CNN) to build a model. 
It is a type of artificial neural network designed for processing and analyzing visual data.
They are characterized by their ability to automatically learn hierarchical features from raw pixel data, making them highly effective for tasks involving images and grids of data.

Step 5: Training the model

In deep learning, before you can train a model, you need to specify several important settings, such as the optimizer, loss function, and evaluation metrics.

Optimizer  -  Adam optimizer to be used during the training of the neural network, because of its efficiency and ability to handle a wide range of neural network architectures.

Loss function -  used to measure the difference between the predicted outputs and the actual target values during training. SparseCategoricalCrossentropy is a convenient and efficient choice for classification tasks where the target labels are integers.

Evaluation metric(s) that will be calculated and displayed during training. Here we use "accuracy" as the metric. Accuracy is a common metric for classification tasks and measures the proportion of correctly classified examples over the total number of examples.

model.fit() function is used to train the model. It includes various arguments :

1.train_ds - dataset object that contains training data.

2. Epochs - defines how many times the model will go through the entire training dataset. Training for multiple epochs allows the model to iteratively improve its performance. 

3. batch_size - specifies the number of training examples that the model processes in each iteration or mini-batch. 

4. verbose: controls the level of output during training.
verbose=0 means no output during training.
verbose=1 provides a progress bar that shows the training progress for each epoch, displaying the training loss and any specified metrics.

5. validation_data: This is typically a dataset object containing your validation data. It is used to evaluate the model's performance during training, but it is not used for training. 

Step 6: Visualization of model output

Plotting the training and validation loss and accuracy of  the model using matplotlib library.

Step 7: Creating a simple user interface

Gradio is a Python library that simplifies the process of building and sharing interactive ML and DL models.
It is used for creating user-friendly web-based interfaces for these models. 
Define a function predict that takes text input and returns predictions from your model.
Then create a Gradio interface using gr.Interface, specifying the input type and the output type.
Finally, you launch the interface, and users can interact with your model via a web browser. 



